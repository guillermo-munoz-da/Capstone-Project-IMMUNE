{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación de textos con Deep Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos el texto y lo pasamos a minuscula\n",
    "filename = \"C:/Users/Josemi/Downloads/caperucita.txt\";\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear mapeo de caracteres únicos a enteros, y un mapeo inverso\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total de caracteres:\", n_chars)\n",
    "print(\"Total vocales:\", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparar el conjunto de datos de entrada para los pares de salida codificados como enteros\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "  seq_in = raw_text[i:i + seq_length]\n",
    "  seq_out = raw_text[i + seq_length]\n",
    "  dataX.append([char_to_int[char] for char in seq_in])\n",
    "  dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total patrones: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remodelar X para que sea [muestras, pasos de tiempo, características]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "#Normalizacion\n",
    "X = X / float(n_vocab)\n",
    "#Codificacion en caliente con la variable de salida\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define el LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se define el checkpoint\n",
    "filepath=\"pesos-los3-30-{epoch:02d}-{loss:.4f}.hdf5\";\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ajuste del modelo\n",
    "model.fit(X, y, epochs=50, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carga de los pesos de la red\n",
    "filename=\"pesos-los3-30-50-1.9956.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Toma una semilla aleatoria\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Semilla:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "#Genera los carácteres\n",
    "for i in range(1000):\n",
    "  x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "  x = x / float(n_vocab)\n",
    "  prediction = model.predict(x, verbose=0)\n",
    "  index = numpy.argmax(prediction)\n",
    "  result = int_to_char[index]\n",
    "  seq_in = [int_to_char[value] for value in pattern]\n",
    "  sys.stdout.write(result)\n",
    "  pattern.append(index)\n",
    "  pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nHecho.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red Neuronal Recurrente LSTM más grande\n",
    "Ahora vamos ha hacer los mismo creando una red mucho más grande. Mantendremos el mismo número de unidades de memoria en 256, pero añadiremos una segunda capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define la LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "filename=\"pesos-los3grande-303-30-2.3081.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grande LSTM Network para generar texto 3 cerditos\n",
    "import sys\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "#Cargamos el texto y lo pasamos a minuscula\n",
    "filename = \"los3.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "#Crear mapeo de caracteres únicos a enteros, y un mapeo inverso\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "#Sumarizamos los datos cargados\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total caracters: \", n_chars)\n",
    "print(\"Total vocabulario: \", n_vocab)\n",
    "#Preparar el conjunto de datos de entrada para los pares de salida codificados como enteros\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "  seq_in = raw_text[i:i + seq_length]\n",
    "  seq_out = raw_text[i + seq_length]\n",
    "  dataX.append([char_to_int[char] for char in seq_in])\n",
    "  dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total patrones: \", n_patterns)\n",
    "#Remodelar X para que sea [muestras, pasos de tiempo, características]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "#Normalizacion\n",
    "X = X / float(n_vocab)\n",
    "#Codificacion en caliente con la variable de salida\n",
    "y = np_utils.to_categorical(dataY)\n",
    "#Se define el LSTM modelo\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "#Se define el checkpoint\n",
    "filepath=\"pesos-los3grandes-303-{epoch:02d}-{loss:.4f}.hdf5\";\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "#Ajuste del modelo\n",
    "model.fit(X, y, epochs=150, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"pesos-los3grandes-303-135-0.0111.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Carga de la red LSTM grande para generar texto\n",
    "import sys\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "#Cargamos el texto y lo pasamos a minuscula\n",
    "filename = \"los3.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "#Crear mapeo de caracteres únicos a enteros, y un mapeo inverso\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "#Sumarizamos los datos cargados\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "#Preparar el conjunto de datos de entrada para los pares de salida codificados como enteros\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "    n_patterns = len(dataX)\n",
    "\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "#Remodelar X para que sea [muestras, pasos de tiempo, características]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "#Normalizacion\n",
    "X = X / float(n_vocab)\n",
    "#Codificacion en caliente de la variable de salida\n",
    "y = np_utils.to_categorical(dataY)\n",
    "#Define la LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "#Carga de los pesos de la red\n",
    "filename=\"pesos-los3grandes-303-135-0.0111.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "#Toma una semilla aleatoria\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Semilla:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "#Genera los carácteres\n",
    "for i in range(1000):\n",
    "  x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "  x = x / float(n_vocab)\n",
    "  prediction = model.predict(x, verbose=0)\n",
    "  index = numpy.argmax(prediction)\n",
    "  result = int_to_char[index]\n",
    "  seq_in = [int_to_char[value] for value in pattern]\n",
    "  sys.stdout.write(result)\n",
    "  pattern.append(index)\n",
    "  pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nHecho.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas de extensión para mejorar el modelo\n",
    "A continuación se muestran ideas que se pueden investigar para mejorar aún más el modelo:\n",
    "\n",
    "- Predecir menos de 1.000 caracteres como salida para una semilla dada.\n",
    "- Eliminar toda la puntuación del texto fuente y, por lo tanto, del vocabulario de los modelos.\n",
    "- Probar con una codificación en caliente para las secuencias de entrada.\n",
    "- Aumentar el número de epoch de entrenamiento a 100 o cientos.\n",
    "- Añadir más unidades de memoria a las capas y/o más capas.\n",
    "- Experimentar con los factores de escala (temperatura) al interpretar las probabilidades de predicción.\n",
    "- Cambiar las capas de LSTM para que estén en estado para mantener el estado en todos los lotes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total caracters:  4705\n",
      "Total vocabulario:  45\n",
      "Total Patterns: 4605\n",
      "Epoch 1/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 3.1456\n",
      "Epoch 1: loss improved from inf to 3.14563, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 38s 478ms/step - loss: 3.1456\n",
      "Epoch 2/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 3.0700\n",
      "Epoch 2: loss improved from 3.14563 to 3.07001, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 38s 526ms/step - loss: 3.0700\n",
      "Epoch 3/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 3.0583\n",
      "Epoch 3: loss improved from 3.07001 to 3.05835, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 42s 580ms/step - loss: 3.0583\n",
      "Epoch 4/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 3.0591\n",
      "Epoch 4: loss did not improve from 3.05835\n",
      "72/72 [==============================] - 42s 582ms/step - loss: 3.0591\n",
      "Epoch 5/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 3.0489\n",
      "Epoch 5: loss improved from 3.05835 to 3.04886, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 41s 572ms/step - loss: 3.0489\n",
      "Epoch 6/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 3.0337\n",
      "Epoch 6: loss improved from 3.04886 to 3.03373, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 42s 583ms/step - loss: 3.0337\n",
      "Epoch 7/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 3.0151\n",
      "Epoch 7: loss improved from 3.03373 to 3.01506, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 42s 587ms/step - loss: 3.0151\n",
      "Epoch 8/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 2.9618\n",
      "Epoch 8: loss improved from 3.01506 to 2.96179, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 41s 573ms/step - loss: 2.9618\n",
      "Epoch 9/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 2.8926\n",
      "Epoch 9: loss improved from 2.96179 to 2.89259, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 41s 573ms/step - loss: 2.8926\n",
      "Epoch 10/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 2.8195\n",
      "Epoch 10: loss improved from 2.89259 to 2.81952, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 41s 564ms/step - loss: 2.8195\n",
      "Epoch 11/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 2.7782\n",
      "Epoch 11: loss improved from 2.81952 to 2.77819, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 41s 565ms/step - loss: 2.7782\n",
      "Epoch 12/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 2.7149\n",
      "Epoch 12: loss improved from 2.77819 to 2.71493, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 41s 562ms/step - loss: 2.7149\n",
      "Epoch 13/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 2.6542\n",
      "Epoch 13: loss improved from 2.71493 to 2.65424, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 41s 571ms/step - loss: 2.6542\n",
      "Epoch 14/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 2.6003\n",
      "Epoch 14: loss improved from 2.65424 to 2.60027, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 41s 575ms/step - loss: 2.6003\n",
      "Epoch 15/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 2.5466\n",
      "Epoch 15: loss improved from 2.60027 to 2.54664, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 41s 573ms/step - loss: 2.5466\n",
      "Epoch 16/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 2.4900\n",
      "Epoch 16: loss improved from 2.54664 to 2.48996, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 41s 573ms/step - loss: 2.4900\n",
      "Epoch 17/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 2.4411\n",
      "Epoch 17: loss improved from 2.48996 to 2.44113, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 42s 589ms/step - loss: 2.4411\n",
      "Epoch 18/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 2.3765\n",
      "Epoch 18: loss improved from 2.44113 to 2.37649, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 43s 597ms/step - loss: 2.3765\n",
      "Epoch 19/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 2.3193\n",
      "Epoch 19: loss improved from 2.37649 to 2.31934, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 42s 581ms/step - loss: 2.3193\n",
      "Epoch 20/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 2.2623\n",
      "Epoch 20: loss improved from 2.31934 to 2.26231, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 42s 579ms/step - loss: 2.2623\n",
      "Epoch 21/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 2.2057\n",
      "Epoch 21: loss improved from 2.26231 to 2.20565, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 42s 587ms/step - loss: 2.2057\n",
      "Epoch 22/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 2.1464\n",
      "Epoch 22: loss improved from 2.20565 to 2.14637, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 41s 571ms/step - loss: 2.1464\n",
      "Epoch 23/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 2.0787\n",
      "Epoch 23: loss improved from 2.14637 to 2.07874, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 42s 582ms/step - loss: 2.0787\n",
      "Epoch 24/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 2.0117\n",
      "Epoch 24: loss improved from 2.07874 to 2.01168, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 42s 579ms/step - loss: 2.0117\n",
      "Epoch 25/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 1.9455\n",
      "Epoch 25: loss improved from 2.01168 to 1.94553, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 41s 571ms/step - loss: 1.9455\n",
      "Epoch 26/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 1.8725\n",
      "Epoch 26: loss improved from 1.94553 to 1.87250, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 41s 574ms/step - loss: 1.8725\n",
      "Epoch 27/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 1.8010\n",
      "Epoch 27: loss improved from 1.87250 to 1.80099, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 42s 580ms/step - loss: 1.8010\n",
      "Epoch 28/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 1.7217\n",
      "Epoch 28: loss improved from 1.80099 to 1.72172, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 42s 576ms/step - loss: 1.7217\n",
      "Epoch 29/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 1.6409\n",
      "Epoch 29: loss improved from 1.72172 to 1.64085, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 41s 568ms/step - loss: 1.6409\n",
      "Epoch 30/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 1.5544\n",
      "Epoch 30: loss improved from 1.64085 to 1.55437, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 40s 549ms/step - loss: 1.5544\n",
      "Epoch 31/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 1.4814\n",
      "Epoch 31: loss improved from 1.55437 to 1.48142, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 39s 537ms/step - loss: 1.4814\n",
      "Epoch 32/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 1.3957\n",
      "Epoch 32: loss improved from 1.48142 to 1.39567, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 41s 567ms/step - loss: 1.3957\n",
      "Epoch 33/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 1.3090\n",
      "Epoch 33: loss improved from 1.39567 to 1.30896, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 42s 578ms/step - loss: 1.3090\n",
      "Epoch 34/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 1.2379\n",
      "Epoch 34: loss improved from 1.30896 to 1.23791, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 41s 565ms/step - loss: 1.2379\n",
      "Epoch 35/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 1.1605\n",
      "Epoch 35: loss improved from 1.23791 to 1.16050, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 40s 562ms/step - loss: 1.1605\n",
      "Epoch 36/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 1.0610\n",
      "Epoch 36: loss improved from 1.16050 to 1.06096, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 41s 571ms/step - loss: 1.0610\n",
      "Epoch 37/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.9747\n",
      "Epoch 37: loss improved from 1.06096 to 0.97468, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 41s 569ms/step - loss: 0.9747\n",
      "Epoch 38/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.9087\n",
      "Epoch 38: loss improved from 0.97468 to 0.90875, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 41s 575ms/step - loss: 0.9087\n",
      "Epoch 39/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.8381\n",
      "Epoch 39: loss improved from 0.90875 to 0.83806, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 43s 602ms/step - loss: 0.8381\n",
      "Epoch 40/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.7699\n",
      "Epoch 40: loss improved from 0.83806 to 0.76993, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 47s 652ms/step - loss: 0.7699\n",
      "Epoch 41/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.7059\n",
      "Epoch 41: loss improved from 0.76993 to 0.70593, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 49s 680ms/step - loss: 0.7059\n",
      "Epoch 42/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.6506\n",
      "Epoch 42: loss improved from 0.70593 to 0.65062, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 48s 669ms/step - loss: 0.6506\n",
      "Epoch 43/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.5967\n",
      "Epoch 43: loss improved from 0.65062 to 0.59673, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 48s 671ms/step - loss: 0.5967\n",
      "Epoch 44/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.5450\n",
      "Epoch 44: loss improved from 0.59673 to 0.54500, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 50s 692ms/step - loss: 0.5450\n",
      "Epoch 45/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.4972\n",
      "Epoch 45: loss improved from 0.54500 to 0.49719, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 49s 675ms/step - loss: 0.4972\n",
      "Epoch 46/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.4495\n",
      "Epoch 46: loss improved from 0.49719 to 0.44949, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 51s 705ms/step - loss: 0.4495\n",
      "Epoch 47/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.4066\n",
      "Epoch 47: loss improved from 0.44949 to 0.40664, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 49s 682ms/step - loss: 0.4066\n",
      "Epoch 48/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.3660\n",
      "Epoch 48: loss improved from 0.40664 to 0.36602, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 49s 680ms/step - loss: 0.3660\n",
      "Epoch 49/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.3350\n",
      "Epoch 49: loss improved from 0.36602 to 0.33498, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 48s 673ms/step - loss: 0.3350\n",
      "Epoch 50/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.3155\n",
      "Epoch 50: loss improved from 0.33498 to 0.31549, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 49s 680ms/step - loss: 0.3155\n",
      "Epoch 51/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.2928\n",
      "Epoch 51: loss improved from 0.31549 to 0.29279, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 51s 708ms/step - loss: 0.2928\n",
      "Epoch 52/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.2544\n",
      "Epoch 52: loss improved from 0.29279 to 0.25441, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 50s 691ms/step - loss: 0.2544\n",
      "Epoch 53/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.2474\n",
      "Epoch 53: loss improved from 0.25441 to 0.24743, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 50s 690ms/step - loss: 0.2474\n",
      "Epoch 54/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.2233\n",
      "Epoch 54: loss improved from 0.24743 to 0.22332, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 51s 703ms/step - loss: 0.2233\n",
      "Epoch 55/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.2110\n",
      "Epoch 55: loss improved from 0.22332 to 0.21102, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 50s 689ms/step - loss: 0.2110\n",
      "Epoch 56/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.1884\n",
      "Epoch 56: loss improved from 0.21102 to 0.18839, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 50s 688ms/step - loss: 0.1884\n",
      "Epoch 57/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.1658\n",
      "Epoch 57: loss improved from 0.18839 to 0.16576, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 48s 670ms/step - loss: 0.1658\n",
      "Epoch 58/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.1492\n",
      "Epoch 58: loss improved from 0.16576 to 0.14917, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 50s 692ms/step - loss: 0.1492\n",
      "Epoch 59/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.1438\n",
      "Epoch 59: loss improved from 0.14917 to 0.14377, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 50s 695ms/step - loss: 0.1438\n",
      "Epoch 60/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.1380\n",
      "Epoch 60: loss improved from 0.14377 to 0.13803, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 50s 687ms/step - loss: 0.1380\n",
      "Epoch 61/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.1249\n",
      "Epoch 61: loss improved from 0.13803 to 0.12489, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 50s 696ms/step - loss: 0.1249\n",
      "Epoch 62/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.1089\n",
      "Epoch 62: loss improved from 0.12489 to 0.10892, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 50s 694ms/step - loss: 0.1089\n",
      "Epoch 63/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.1091\n",
      "Epoch 63: loss did not improve from 0.10892\n",
      "72/72 [==============================] - 50s 691ms/step - loss: 0.1091\n",
      "Epoch 64/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.1065\n",
      "Epoch 64: loss improved from 0.10892 to 0.10653, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 51s 702ms/step - loss: 0.1065\n",
      "Epoch 65/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 65: loss improved from 0.10653 to 0.09808, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 50s 693ms/step - loss: 0.0981\n",
      "Epoch 66/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0874\n",
      "Epoch 66: loss improved from 0.09808 to 0.08739, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 50s 689ms/step - loss: 0.0874\n",
      "Epoch 67/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0942\n",
      "Epoch 67: loss did not improve from 0.08739\n",
      "72/72 [==============================] - 50s 700ms/step - loss: 0.0942\n",
      "Epoch 68/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0779\n",
      "Epoch 68: loss improved from 0.08739 to 0.07787, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 49s 687ms/step - loss: 0.0779\n",
      "Epoch 69/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0722\n",
      "Epoch 69: loss improved from 0.07787 to 0.07219, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 50s 690ms/step - loss: 0.0722\n",
      "Epoch 70/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0755\n",
      "Epoch 70: loss did not improve from 0.07219\n",
      "72/72 [==============================] - 49s 683ms/step - loss: 0.0755\n",
      "Epoch 71/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0648\n",
      "Epoch 71: loss improved from 0.07219 to 0.06484, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 49s 679ms/step - loss: 0.0648\n",
      "Epoch 72/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0577\n",
      "Epoch 72: loss improved from 0.06484 to 0.05766, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 49s 676ms/step - loss: 0.0577\n",
      "Epoch 73/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0656\n",
      "Epoch 73: loss did not improve from 0.05766\n",
      "72/72 [==============================] - 49s 685ms/step - loss: 0.0656\n",
      "Epoch 74/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0794\n",
      "Epoch 74: loss did not improve from 0.05766\n",
      "72/72 [==============================] - 50s 689ms/step - loss: 0.0794\n",
      "Epoch 75/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0727\n",
      "Epoch 75: loss did not improve from 0.05766\n",
      "72/72 [==============================] - 51s 704ms/step - loss: 0.0727\n",
      "Epoch 76/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0648\n",
      "Epoch 76: loss did not improve from 0.05766\n",
      "72/72 [==============================] - 50s 694ms/step - loss: 0.0648\n",
      "Epoch 77/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0570\n",
      "Epoch 77: loss improved from 0.05766 to 0.05701, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 50s 689ms/step - loss: 0.0570\n",
      "Epoch 78/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0498\n",
      "Epoch 78: loss improved from 0.05701 to 0.04980, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 51s 707ms/step - loss: 0.0498\n",
      "Epoch 79/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0453\n",
      "Epoch 79: loss improved from 0.04980 to 0.04534, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 50s 699ms/step - loss: 0.0453\n",
      "Epoch 80/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0443\n",
      "Epoch 80: loss improved from 0.04534 to 0.04426, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 51s 704ms/step - loss: 0.0443\n",
      "Epoch 81/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0456\n",
      "Epoch 81: loss did not improve from 0.04426\n",
      "72/72 [==============================] - 51s 708ms/step - loss: 0.0456\n",
      "Epoch 82/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0525\n",
      "Epoch 82: loss did not improve from 0.04426\n",
      "72/72 [==============================] - 51s 715ms/step - loss: 0.0525\n",
      "Epoch 83/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0733\n",
      "Epoch 83: loss did not improve from 0.04426\n",
      "72/72 [==============================] - 51s 714ms/step - loss: 0.0733\n",
      "Epoch 84/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0609\n",
      "Epoch 84: loss did not improve from 0.04426\n",
      "72/72 [==============================] - 52s 720ms/step - loss: 0.0609\n",
      "Epoch 85/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0533\n",
      "Epoch 85: loss did not improve from 0.04426\n",
      "72/72 [==============================] - 51s 713ms/step - loss: 0.0533\n",
      "Epoch 86/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0349\n",
      "Epoch 86: loss improved from 0.04426 to 0.03487, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 50s 694ms/step - loss: 0.0349\n",
      "Epoch 87/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0366\n",
      "Epoch 87: loss did not improve from 0.03487\n",
      "72/72 [==============================] - 51s 703ms/step - loss: 0.0366\n",
      "Epoch 88/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0311\n",
      "Epoch 88: loss improved from 0.03487 to 0.03110, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 51s 705ms/step - loss: 0.0311\n",
      "Epoch 89/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0302\n",
      "Epoch 89: loss improved from 0.03110 to 0.03023, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 51s 706ms/step - loss: 0.0302\n",
      "Epoch 90/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0339\n",
      "Epoch 90: loss did not improve from 0.03023\n",
      "72/72 [==============================] - 51s 702ms/step - loss: 0.0339\n",
      "Epoch 91/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0420\n",
      "Epoch 91: loss did not improve from 0.03023\n",
      "72/72 [==============================] - 51s 713ms/step - loss: 0.0420\n",
      "Epoch 92/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0338\n",
      "Epoch 92: loss did not improve from 0.03023\n",
      "72/72 [==============================] - 52s 723ms/step - loss: 0.0338\n",
      "Epoch 93/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0260\n",
      "Epoch 93: loss improved from 0.03023 to 0.02599, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 52s 720ms/step - loss: 0.0260\n",
      "Epoch 94/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0238\n",
      "Epoch 94: loss improved from 0.02599 to 0.02384, saving model to model_weights_saved.hdf5\n",
      "72/72 [==============================] - 51s 705ms/step - loss: 0.0238\n",
      "Epoch 95/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0267\n",
      "Epoch 95: loss did not improve from 0.02384\n",
      "72/72 [==============================] - 51s 703ms/step - loss: 0.0267\n",
      "Epoch 96/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0255\n",
      "Epoch 96: loss did not improve from 0.02384\n",
      "72/72 [==============================] - 51s 705ms/step - loss: 0.0255\n",
      "Epoch 97/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0651\n",
      "Epoch 97: loss did not improve from 0.02384\n",
      "72/72 [==============================] - 51s 714ms/step - loss: 0.0651\n",
      "Epoch 98/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.1206\n",
      "Epoch 98: loss did not improve from 0.02384\n",
      "72/72 [==============================] - 51s 714ms/step - loss: 0.1206\n",
      "Epoch 99/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0682\n",
      "Epoch 99: loss did not improve from 0.02384\n",
      "72/72 [==============================] - 52s 718ms/step - loss: 0.0682\n",
      "Epoch 100/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0468\n",
      "Epoch 100: loss did not improve from 0.02384\n",
      "72/72 [==============================] - 51s 716ms/step - loss: 0.0468\n",
      "Epoch 101/101\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.0365\n",
      "Epoch 101: loss did not improve from 0.02384\n",
      "72/72 [==============================] - 51s 713ms/step - loss: 0.0365\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2175cb3f8b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import sys\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filename = \"caperucita.txt\"\n",
    "raw_text = open(filename, encoding=\"utf-8\",errors=\"ignore\").read()\n",
    "\n",
    "\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "#Crear mapeo de caracteres únicos a enteros, y un mapeo inverso\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "#Sumarizamos los datos cargados\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total caracters: \", n_chars)\n",
    "print(\"Total vocabulario: \", n_vocab)\n",
    "\n",
    "seq_length = 100\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "# loop through inputs, start at the beginning and go until we hit\n",
    "# the final character we can create a sequence out of\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    # Define input and output sequences\n",
    "    # Input is the current character plus desired sequence length\n",
    "    in_seq = raw_text[i:i + seq_length]\n",
    "    \n",
    "    \n",
    "    # Out sequence is the initial character plus total sequence length\n",
    "    out_seq = raw_text[i + seq_length]\n",
    "\n",
    "    # We now convert list of characters to integers based on\n",
    "    # previously and add the values to our lists\n",
    "    x_data.append([char_to_int[char] for char in in_seq])\n",
    "    \n",
    "    y_data.append(char_to_int[out_seq])\n",
    "\n",
    "n_patterns = len(x_data)    \n",
    "print (\"Total Patterns:\", n_patterns)\n",
    "y = np_utils.to_categorical(y_data)\n",
    "X = np.reshape(x_data, (n_patterns, seq_length, 1))\n",
    "\n",
    "#shape de la matriz encoded= (3129, 100, 39)\n",
    "\n",
    "X = X/float(n_vocab)\n",
    "#Define la LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], 1), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "#Carga de los pesos de la red\n",
    "filepath = \"model_weights_saved.hdf5\"\n",
    "\n",
    "Checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [Checkpoint]\n",
    "#Ajuste del modelo\n",
    "model.fit(X, y, epochs=101, batch_size=64, callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semilla:\n",
      "\" e directo a la cama de la abuelita y de un bocado se la tragã³. y enseguida se puso ropa de ella, se \"\n",
      " colocã³ un gorro, se metiã³ en la cama y cerrã³ las cortinas.\n",
      "\n",
      "mientras tanto, caperucita roja se habã­a quedado colectando flores, y cuando vio que tenã­a tantas que ya no podã­a llevar mã¡s, se acordã³ de su abuelita y se puso en camino hacia ella. cuando llegã³, se sorprendiã³ a  sano hobã³a tartan a la casa, y con una apariencia muy extraã±a. \"â¡!oh, abuelita!\" dijo, \"quã© orejas tan grandes que tienes.\" - \"es para oã­rte mejor, mi niã±a,\" fue la respuesta. \"pero abuelita, quã© ojos tan grandes que tienes.\" - \"son para verte mejor, querida.\" - \"pero abuelita, quã© brazos tan grandes que tienes.\" - \"para abrazarte mejor.\" - \"y quã© boca tan grande que tienes.\" - \"para comerte mejor.\" y no habã­a terminado de decir lo anterior, cuando de un salto saliã³ de la cama y se tragã³ tambiã©n a caperucita roja.\n",
      "\n",
      "y colorã­n colorado, este cuento se ha acabado. q no abueltta y de un bocado se la tragã³. y enseguida se puso ropa de ella, se colocã³ un gorro, se metiã³ en la cama y cerrã³ las c\n",
      "Hecho.\n"
     ]
    }
   ],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "filename=\"model_weights_saved.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "#Toma una semilla aleatoria\n",
    "start = np.random.randint(0, len(x_data)-1)\n",
    "pattern = x_data[start]\n",
    "print(\"Semilla:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "#Genera los carácteres\n",
    "for i in range(1000):\n",
    "  x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "  x = x / float(n_vocab)\n",
    "  prediction = model.predict(x, verbose=0)\n",
    "  index = np.argmax(prediction)\n",
    "  result = int_to_char[index]\n",
    "  seq_in = [int_to_char[value] for value in pattern]\n",
    "  sys.stdout.write(result)\n",
    "  pattern.append(index)\n",
    "  pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nHecho.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
